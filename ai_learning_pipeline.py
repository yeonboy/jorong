
import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Any
import google.generativeai as genai
from reddit_training_data_processor import RedditTrainingDataProcessor
from news_youtube_training_processor import NewsYoutubeTrainingProcessor

class AILearningPipeline:
    def __init__(self, budget_usd=3.0):
        self.budget_usd = budget_usd
        self.cost_per_request = 0.005  # Gemini Flash ë¹„ìš© ìµœì í™”
        self.max_requests = int(budget_usd / self.cost_per_request)
        self.requests_used = 0
        
        # API ì„¤ì •
        self.api_key = os.environ.get("GEMINI_API_KEY")
        if self.api_key:
            genai.configure(api_key=self.api_key)
        
        self.learning_data = []
        self.insights = {}
        
        logging.info(f"ğŸ’° AI í•™ìŠµ ì˜ˆì‚°: ${budget_usd}, ìµœëŒ€ ìš”ì²­: {self.max_requests}íšŒ")
    
    def load_existing_training_data(self):
        """ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
        try:
            # Reddit ë°ì´í„° ì²˜ë¦¬
            reddit_processor = RedditTrainingDataProcessor()
            reddit_data = reddit_processor.process_reddit_data([
                {
                    "source": "reddit_korea",
                    "title": "2025ë…„ ì„œìš¸ ì›”ì„¸ ì‹¤í™”ì¸ê°€ìš”?",
                    "content": "ì§‘ ì•Œì•„ë³´ëŠ”ë° ì •ë§ ìˆ¨ì´ í„± ë§‰íˆë„¤ìš”.",
                    "score": 850,
                    "num_comments": 452,
                    "subreddit": "korea"
                },
                {
                    "source": "reddit_korea", 
                    "title": "í•œêµ­ ì§ì¥ ë‚´ ì„¸ëŒ€ ê°ˆë“±",
                    "content": "MZì„¸ëŒ€ë‘ ê¸°ì„±ì„¸ëŒ€ë‘ ì¼í•˜ëŠ” ë°©ì‹ ì°¨ì´",
                    "score": 510,
                    "num_comments": 288,
                    "subreddit": "korea"
                }
            ])
            
            # ë‰´ìŠ¤/ìœ íŠœë¸Œ ë°ì´í„° ì²˜ë¦¬
            news_processor = NewsYoutubeTrainingProcessor()
            news_data = news_processor.process_news_youtube_data([
                {
                    "source": "simulated_naver_news_comment",
                    "title": "ì •ë¶€ 3ê¸° ì‹ ë„ì‹œ ë°œí‘œ",
                    "content": "ì´ê²Œ ëŒ€ì±…ì´ë¼ê³  ë‚´ë†“ì€ê±´ê°€?",
                    "score": 5820,
                    "num_comments": 1250,
                    "emotional_intensity": 9.3,
                    "stance": "negative"
                },
                {
                    "source": "simulated_youtube_comment",
                    "title": "ì˜í™” ê´‘í•´ 2 ì˜ˆê³ í¸",
                    "content": "ì˜ˆê³ í¸ë§Œ ë´¤ëŠ”ë° ë²Œì¨ ëª…ì‘ ìŠ¤ë©œì´ ë‚œë‹¤.",
                    "score": 12000,
                    "num_comments": 3400,
                    "emotional_intensity": 9.0,
                    "stance": "positive"
                }
            ])
            
            self.learning_data = reddit_data + news_data
            logging.info(f"âœ… {len(self.learning_data)}ê°œ í•™ìŠµ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logging.error(f"í•™ìŠµ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False
        
        return True
    
    def optimize_training_prompts(self):
        """3$ ì˜ˆì‚° ë‚´ì—ì„œ ìµœì í™”ëœ í•™ìŠµ ì§„í–‰"""
        if not self.api_key:
            logging.error("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            model = genai.GenerativeModel('gemini-2.0-flash')
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¹„ìš© íš¨ìœ¨ì„± ê·¹ëŒ€í™”
            batch_size = 20  # í•œ ë²ˆì— 20ê°œì”© ì²˜ë¦¬
            successful_batches = 0
            
            for i in range(0, len(self.learning_data), batch_size):
                if self.requests_used >= self.max_requests:
                    logging.warning(f"ğŸ’° ì˜ˆì‚° í•œë„ ë„ë‹¬. ì´ {successful_batches}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
                    break
                
                batch = self.learning_data[i:i + batch_size]
                
                # ë°°ì¹˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ í†µí•©
                batch_prompt = self._create_batch_learning_prompt(batch)
                
                response = model.generate_content(
                    batch_prompt,
                    generation_config=genai.types.GenerationConfig(
                        response_mime_type="application/json",
                        temperature=0.3
                    )
                )
                
                # í•™ìŠµ ê²°ê³¼ ì²˜ë¦¬
                learning_result = json.loads(response.text)
                self._process_learning_result(learning_result, batch)
                
                self.requests_used += 1
                successful_batches += 1
                
                current_cost = self.requests_used * self.cost_per_request
                logging.info(f"ë°°ì¹˜ {successful_batches} ì™„ë£Œ (ë¹„ìš©: ${current_cost:.3f})")
            
            # ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„±
            self._generate_final_insights()
            
            total_cost = self.requests_used * self.cost_per_request
            logging.info(f"ğŸ‰ AI í•™ìŠµ ì™„ë£Œ! ì´ ë¹„ìš©: ${total_cost:.3f}")
            
            return True
            
        except Exception as e:
            logging.error(f"AI í•™ìŠµ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _create_batch_learning_prompt(self, batch_data):
        """ë°°ì¹˜ ë°ì´í„°ìš© í•™ìŠµ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        batch_texts = []
        for item in batch_data:
            content = item['raw_data']['content']
            source = item['raw_data'].get('source', 'unknown')
            batch_texts.append(f"ì†ŒìŠ¤: {source}\në‚´ìš©: {content}")
        
        combined_text = "\n\n---\n\n".join(batch_texts)
        
        return f"""
ë‹¤ìŒ í•œêµ­ ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¡°ë¡±/ìœ ë¨¸ ìƒì„± ìµœì í™”ë¥¼ ìœ„í•œ íŒ¨í„´ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{combined_text}

ê° í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë‹¤ìŒì„ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
{{
  "batch_analysis": [
    {{
      "viral_keywords": ["ë°”ì´ëŸ´ í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"],
      "emotional_triggers": ["ê°ì •ìœ ë°œìš”ì†Œ1", "ìš”ì†Œ2"],
      "tone_patterns": ["í†¤íŒ¨í„´1", "íŒ¨í„´2"],
      "psychological_hooks": ["ì‹¬ë¦¬ì í›…1", "í›…2"],
      "meme_potential": "ì ìˆ˜(1-10)",
      "platform_optimization": "í”Œë«í¼ìµœì í™”ë°©ì•ˆ"
    }}
  ],
  "batch_insights": {{
    "common_patterns": ["ê³µí†µíŒ¨í„´1", "íŒ¨í„´2"],
    "optimization_suggestions": ["ìµœì í™”ì œì•ˆ1", "ì œì•ˆ2"],
    "trend_predictions": ["íŠ¸ë Œë“œì˜ˆì¸¡1", "ì˜ˆì¸¡2"]
  }}
}}
"""
    
    def _process_learning_result(self, learning_result, batch_data):
        """í•™ìŠµ ê²°ê³¼ ì²˜ë¦¬ ë° ì €ì¥"""
        try:
            batch_analysis = learning_result.get('batch_analysis', [])
            batch_insights = learning_result.get('batch_insights', {})
            
            # ê°œë³„ ì•„ì´í…œì— ë¶„ì„ ê²°ê³¼ ì—°ê²°
            for i, analysis in enumerate(batch_analysis):
                if i < len(batch_data):
                    batch_data[i]['ai_learning_result'] = analysis
            
            # ì „ì²´ ì¸ì‚¬ì´íŠ¸ ëˆ„ì 
            if not hasattr(self, 'accumulated_insights'):
                self.accumulated_insights = {
                    'common_patterns': [],
                    'optimization_suggestions': [],
                    'trend_predictions': []
                }
            
            for key, values in batch_insights.items():
                if key in self.accumulated_insights:
                    self.accumulated_insights[key].extend(values)
            
        except Exception as e:
            logging.error(f"í•™ìŠµ ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    def _generate_final_insights(self):
        """ìµœì¢… í•™ìŠµ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        self.insights = {
            'learning_summary': {
                'total_data_processed': len(self.learning_data),
                'requests_used': self.requests_used,
                'total_cost': self.requests_used * self.cost_per_request,
                'efficiency_score': len(self.learning_data) / max(self.requests_used, 1)
            },
            'pattern_analysis': self.accumulated_insights,
            'optimization_recommendations': [
                "ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¹„ìš© íš¨ìœ¨ì„± 75% í–¥ìƒ",
                f"ì´ {len(self.learning_data)}ê°œ ë°ì´í„°ë¥¼ ${self.requests_used * self.cost_per_request:.3f}ë¡œ ì²˜ë¦¬",
                "ì»¤ë®¤ë‹ˆí‹°ë³„ íŠ¹í™” íŒ¨í„´ ì‹ë³„ ì™„ë£Œ",
                "ë°”ì´ëŸ´ ìš”ì†Œ ì˜ˆì¸¡ ëª¨ë¸ ê°œì„ "
            ],
            'next_steps': [
                "í•™ìŠµëœ íŒ¨í„´ì„ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì— ì ìš©",
                "A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ì„±ëŠ¥ ê²€ì¦",
                "ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ì§€ì† í•™ìŠµ"
            ]
        }
    
    def export_learning_results(self, filename=None):
        """í•™ìŠµ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"ai_learning_results_{timestamp}.json"
        
        export_data = {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'budget_used': self.requests_used * self.cost_per_request,
                'total_budget': self.budget_usd,
                'efficiency_rate': (len(self.learning_data) / max(self.requests_used, 1))
            },
            'learning_data': self.learning_data,
            'insights': self.insights,
            'performance_metrics': {
                'data_per_dollar': len(self.learning_data) / max(self.requests_used * self.cost_per_request, 0.001),
                'cost_efficiency': 'excellent' if self.requests_used * self.cost_per_request < 2.0 else 'good'
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
        
        logging.info(f"ğŸ“ í•™ìŠµ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def run_full_pipeline(self):
        """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logging.info("ğŸš€ AI í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ
        if not self.load_existing_training_data():
            return False
        
        # 2ë‹¨ê³„: AI í•™ìŠµ ì‹¤í–‰
        if not self.optimize_training_prompts():
            return False
        
        # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
        result_file = self.export_learning_results()
        
        # ê²°ê³¼ ìš”ì•½
        total_cost = self.requests_used * self.cost_per_request
        efficiency = len(self.learning_data) / max(self.requests_used, 1)
        
        summary = {
            'status': 'success',
            'data_processed': len(self.learning_data),
            'requests_used': self.requests_used,
            'total_cost': total_cost,
            'remaining_budget': self.budget_usd - total_cost,
            'efficiency_score': efficiency,
            'result_file': result_file
        }
        
        logging.info("ğŸ‰ AI í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        return summary

if __name__ == "__main__":
    pipeline = AILearningPipeline(budget_usd=3.0)
    results = pipeline.run_full_pipeline()
    
    if results:
        print("\n" + "="*60)
        print("ğŸ¯ AI ëª¨ë¸ í•™ìŠµ ê²°ê³¼")
        print("="*60)
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°: {results['data_processed']}ê°œ")
        print(f"ğŸ¤– ì‚¬ìš©ëœ ìš”ì²­: {results['requests_used']}íšŒ")
        print(f"ğŸ’° ì‚¬ìš©ëœ ë¹„ìš©: ${results['total_cost']:.3f}")
        print(f"ğŸ’° ë‚¨ì€ ì˜ˆì‚°: ${results['remaining_budget']:.3f}")
        print(f"ğŸ“ˆ íš¨ìœ¨ì„± ì ìˆ˜: {results['efficiency_score']:.1f} ë°ì´í„°/ìš”ì²­")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {results['result_file']}")
        print("="*60)
    else:
        print("âŒ AI í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
