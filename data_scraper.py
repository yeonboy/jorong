
import requests
import json
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any
import re
from database_setup import TauntResearchDB
import google.generativeai as genai
import os
import logging

class KoreanCommunityDataScraper:
    """êµ­ë‚´ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìŠ¤í¬ë˜í•‘ ë° AI í•™ìŠµ ë°ì´í„° ìƒì„±"""
    
    def __init__(self):
        self.db = TauntResearchDB()
        self.gemini_api_key = os.environ.get("GEMINI_API_KEY")
        if self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
        
        # ê³µê°œ API ê¸°ë°˜ ë°ì´í„° ì†ŒìŠ¤
        self.data_sources = {
            "reddit_korean": {
                "api_endpoint": "https://www.reddit.com/r/korea.json",
                "headers": {"User-Agent": "Korean Community Research Bot 1.0"},
                "rate_limit": 1,  # ì´ˆë‹¹ ìš”ì²­ ìˆ˜
                "data_type": "reddit_posts"
            },
            "public_feeds": {
                "naver_news": "https://openapi.naver.com/v1/search/news.json",
                "data_type": "news_comments"
            }
        }
        
        # ë¹„ìš© íš¨ìœ¨ì  AI ì‚¬ìš©ì„ ìœ„í•œ ì„¤ì •
        self.ai_usage_budget = 4.0  # ë‹¬ëŸ¬
        self.cost_per_request = 0.01  # ì¶”ì • ë¹„ìš©
        self.max_requests = int(self.ai_usage_budget / self.cost_per_request)
        self.requests_used = 0
    
    def scrape_public_korean_data(self):
        """ê³µê°œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ í•œêµ­ì–´ ì½˜í…ì¸  ìˆ˜ì§‘"""
        
        scraped_data = []
        
        try:
            # Reddit í•œêµ­ ê´€ë ¨ ì„œë¸Œë ˆë”§ì—ì„œ ê³µê°œ ë°ì´í„° ìˆ˜ì§‘
            logging.info("ğŸ” Reddit í•œêµ­ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
            
            response = requests.get(
                self.data_sources["reddit_korean"]["api_endpoint"],
                headers=self.data_sources["reddit_korean"]["headers"]
            )
            
            if response.status_code == 200:
                reddit_data = response.json()
                posts = reddit_data.get('data', {}).get('children', [])
                
                for post in posts[:50]:  # ìµœëŒ€ 50ê°œ í¬ìŠ¤íŠ¸
                    post_data = post.get('data', {})
                    
                    # í•œêµ­ì–´ í…ìŠ¤íŠ¸ í•„í„°ë§
                    title = post_data.get('title', '')
                    selftext = post_data.get('selftext', '')
                    
                    if self._contains_korean(title) or self._contains_korean(selftext):
                        scraped_data.append({
                            'source': 'reddit_korea',
                            'title': title,
                            'content': selftext,
                            'score': post_data.get('score', 0),
                            'num_comments': post_data.get('num_comments', 0),
                            'created_utc': post_data.get('created_utc', 0),
                            'url': post_data.get('url', ''),
                            'subreddit': post_data.get('subreddit', ''),
                            'data_type': 'community_post'
                        })
                
                logging.info(f"âœ… Redditì—ì„œ {len([d for d in scraped_data if d['source'] == 'reddit_korea'])}ê°œ í•œêµ­ì–´ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘")
            
            # ì‹œë®¬ë ˆì´ì…˜ëœ êµ­ë‚´ ì»¤ë®¤ë‹ˆí‹° íŒ¨í„´ ë°ì´í„° ìƒì„±
            simulated_data = self._generate_simulated_korean_community_data()
            scraped_data.extend(simulated_data)
            
            return scraped_data
            
        except Exception as e:
            logging.error(f"ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
            # ì˜¤ë¥˜ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ëŒ€ì²´
            return self._generate_simulated_korean_community_data()
    
    def _contains_korean(self, text):
        """í…ìŠ¤íŠ¸ì— í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        korean_pattern = re.compile(r'[ã„±-ã…ã…-ã…£ê°€-í£]')
        return bool(korean_pattern.search(text))
    
    def _generate_simulated_korean_community_data(self):
        """ì‹œë®¬ë ˆì´ì…˜ëœ êµ­ë‚´ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìƒì„± (ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ëŒ€ì²´ìš©)"""
        
        simulated_patterns = [
            {
                'source': 'simulated_theqoo',
                'title': 'ì´ê±° ì§„ì§œ ê°œì›ƒê¸°ë„¤ ã…‹ã…‹ã…‹',
                'content': 'ì™„ì „ ë ˆì „ë“œ ì•„ë‹ˆëƒ ë¯¸ì³¤ë‹¤ ì§„ì§œë¡œ',
                'score': 156,
                'num_comments': 23,
                'data_type': 'viral_reaction',
                'speech_pattern': 'theqoo_style',
                'emotional_intensity': 8.5
            },
            {
                'source': 'simulated_mlbpark',
                'title': 'ì´ê±° ì§„ì§œ íŒ©íŠ¸ì„?',
                'content': 'ê°ê´€ì ìœ¼ë¡œ ë´¤ì„ ë•Œ ì´í•´ê°€ ì•ˆ ê°. ê·¼ê±°ê°€ ìˆë‚˜?',
                'score': 89,
                'num_comments': 45,
                'data_type': 'logical_debate',
                'speech_pattern': 'mlbpark_style',
                'emotional_intensity': 6.2
            },
            {
                'source': 'simulated_instiz',
                'title': 'ì™„ì „ ì‹¬ì¿µ í¬ì¸íŠ¸',
                'content': 'ì´ê±° ì‹¤í™”ëƒ í— ì™„ì „ ë‚´ ì·¨í–¥ì €ê²©',
                'score': 234,
                'num_comments': 67,
                'data_type': 'fandom_reaction',
                'speech_pattern': 'instiz_style',
                'emotional_intensity': 9.1
            },
            {
                'source': 'simulated_dc',
                'title': 'ã…‹ã…‹ã…‹ã…‹ ê°œì›ƒê¸°ë„¤',
                'content': 'ì´ê±° ã„¹ã…‡ ì°ì„? ë¯¸ì¹œë†ˆì´ë„¤ ã…‹ã…‹ã…‹',
                'score': 78,
                'num_comments': 123,
                'data_type': 'anonymous_humor',
                'speech_pattern': 'dc_style',
                'emotional_intensity': 7.8
            },
            {
                'source': 'simulated_pann',
                'title': 'ì´ê±° ì§„ì§œ ì¶©ê²©ì ì´ë‹¤',
                'content': 'ì™„ì „ ë°˜ì „ ì•„ë‹ˆì•¼? ì´ëŸ° ì¼ì´ ì‹¤ì œë¡œ?',
                'score': 445,
                'num_comments': 89,
                'data_type': 'gossip_reaction',
                'speech_pattern': 'pann_style',
                'emotional_intensity': 8.9
            }
        ]
        
        # íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ë” ë§ì€ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
        expanded_data = []
        for _ in range(100):  # 100ê°œì˜ ì¶”ê°€ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
            base_pattern = random.choice(simulated_patterns)
            variation = base_pattern.copy()
            variation['created_utc'] = datetime.now().timestamp() - random.randint(0, 86400)
            variation['score'] = random.randint(10, 500)
            variation['num_comments'] = random.randint(5, 200)
            expanded_data.append(variation)
        
        return simulated_patterns + expanded_data
    
    def analyze_scraped_data_with_ai(self, scraped_data):
        """ìŠ¤í¬ë˜í•‘ëœ ë°ì´í„°ë¥¼ AIë¡œ ë¶„ì„í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìƒì„±"""
        
        if not self.gemini_api_key:
            logging.warning("âš ï¸ Gemini API í‚¤ê°€ ì—†ì–´ ì‹œë®¬ë ˆì´ì…˜ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            return self._simulate_ai_analysis(scraped_data)
        
        analyzed_results = []
        
        try:
            model = genai.GenerativeModel('gemini-2.0-flash')
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¹„ìš© íš¨ìœ¨ì„± ê·¹ëŒ€í™”
            batch_size = 10
            batches = [scraped_data[i:i+batch_size] for i in range(0, len(scraped_data), batch_size)]
            
            for batch_idx, batch in enumerate(batches):
                if self.requests_used >= self.max_requests:
                    logging.warning(f"ğŸ’° AI ì‚¬ìš© ì˜ˆì‚° ({self.ai_usage_budget}$) ì†Œì§„ìœ¼ë¡œ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                
                # ë°°ì¹˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ í†µí•© ì²˜ë¦¬
                batch_text = "\n\n".join([
                    f"ì œëª©: {item['title']}\në‚´ìš©: {item['content']}\ní”Œë«í¼: {item['source']}"
                    for item in batch
                ])
                
                analysis_prompt = f"""
ë‹¤ìŒ í•œêµ­ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê°ê°ì— ëŒ€í•´ ì¡°ë¡±/ìœ ë¨¸ ìƒì„±ì— í™œìš©í•  ìˆ˜ ìˆëŠ” íŒ¨í„´ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{batch_text}

ê° ë°ì´í„°ì— ëŒ€í•´ ë‹¤ìŒì„ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
{{
  "speech_patterns": ["ê°ì •ê°•í™”ì–´", "ë°˜ì‘íŒ¨í„´", "íŠ¹ì§•ì í‘œí˜„"],
  "emotional_hooks": ["ìš°ì›”ê°ìê·¹", "ê³µê°ëŒ€í˜•ì„±", "í˜¸ê¸°ì‹¬ìœ ë°œ"],
  "viral_elements": ["ë°”ì´ëŸ´ìš”ì†Œ1", "ë°”ì´ëŸ´ìš”ì†Œ2"],
  "psychological_mechanisms": "ì‹¬ë¦¬ì  ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…",
  "tone_classification": "í†¤ ë¶„ë¥˜",
  "effectiveness_score": ì ìˆ˜(1-10),
  "usage_recommendations": "í™œìš© ê¶Œì¥ì‚¬í•­"
}}
"""
                
                response = model.generate_content(
                    analysis_prompt,
                    generation_config=genai.types.GenerationConfig(
                        response_mime_type="application/json",
                        temperature=0.3
                    )
                )
                
                try:
                    batch_analysis = json.loads(response.text)
                    
                    # ë°°ì¹˜ ê²°ê³¼ë¥¼ ê°œë³„ ì•„ì´í…œì— í• ë‹¹
                    for i, item in enumerate(batch):
                        item_analysis = batch_analysis if isinstance(batch_analysis, dict) else batch_analysis[i] if i < len(batch_analysis) else {}
                        
                        analyzed_results.append({
                            'original_data': item,
                            'ai_analysis': item_analysis,
                            'analysis_date': datetime.now().isoformat(),
                            'cost_used': self.cost_per_request / batch_size
                        })
                    
                    self.requests_used += 1
                    logging.info(f"âœ… ë°°ì¹˜ {batch_idx + 1}/{len(batches)} ë¶„ì„ ì™„ë£Œ (ë¹„ìš©: ${self.requests_used * self.cost_per_request:.2f})")
                    
                    # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                    time.sleep(1)
                    
                except json.JSONDecodeError:
                    logging.error(f"ë°°ì¹˜ {batch_idx + 1} JSON íŒŒì‹± ì‹¤íŒ¨")
                    continue
            
            logging.info(f"ğŸ¯ ì´ {len(analyzed_results)}ê°œ ë°ì´í„° ë¶„ì„ ì™„ë£Œ (ì´ ë¹„ìš©: ${self.requests_used * self.cost_per_request:.2f})")
            return analyzed_results
            
        except Exception as e:
            logging.error(f"AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return self._simulate_ai_analysis(scraped_data)
    
    def _simulate_ai_analysis(self, scraped_data):
        """AI ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ (API í‚¤ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ì‹œ ì‚¬ìš©)"""
        
        simulated_analysis = []
        
        for item in scraped_data:
            analysis = {
                'original_data': item,
                'ai_analysis': {
                    'speech_patterns': self._extract_speech_patterns(item),
                    'emotional_hooks': self._identify_emotional_hooks(item),
                    'viral_elements': self._find_viral_elements(item),
                    'psychological_mechanisms': 'ì‹œë®¬ë ˆì´ì…˜ëœ ì‹¬ë¦¬ì  ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„',
                    'tone_classification': item.get('speech_pattern', 'ì¼ë°˜í†¤'),
                    'effectiveness_score': random.uniform(6.0, 9.5),
                    'usage_recommendations': 'ì‹œë®¬ë ˆì´ì…˜ëœ í™œìš© ê¶Œì¥ì‚¬í•­'
                },
                'analysis_date': datetime.now().isoformat(),
                'cost_used': 0.0  # ì‹œë®¬ë ˆì´ì…˜ì´ë¯€ë¡œ ë¹„ìš© ì—†ìŒ
            }
            simulated_analysis.append(analysis)
        
        return simulated_analysis
    
    def _extract_speech_patterns(self, item):
        """í…ìŠ¤íŠ¸ì—ì„œ í™”ë²• íŒ¨í„´ ì¶”ì¶œ"""
        text = f"{item.get('title', '')} {item.get('content', '')}"
        patterns = []
        
        if 'ã…‹ã…‹' in text: patterns.append('ì›ƒìŒí‘œí˜„')
        if any(word in text for word in ['ì§„ì§œ', 'ì™„ì „', 'ê°œ']): patterns.append('ê°•í™”ì–´')
        if any(word in text for word in ['ë¯¸ì³¤ë‹¤', 'ë ˆì „ë“œ', 'ëŒ€ë°•']): patterns.append('ê·¹ì°¬í‘œí˜„')
        if '?' in text: patterns.append('ì˜ë¬¸í˜•')
        
        return patterns or ['ì¼ë°˜íŒ¨í„´']
    
    def _identify_emotional_hooks(self, item):
        """ê°ì •ì  ìê·¹ ìš”ì†Œ ì‹ë³„"""
        text = f"{item.get('title', '')} {item.get('content', '')}"
        hooks = []
        
        score = item.get('score', 0)
        if score > 100: hooks.append('ê³µê°ëŒ€í˜•ì„±')
        if any(word in text for word in ['ì¶©ê²©', 'ë°˜ì „', 'ì„¤ë§ˆ']): hooks.append('í˜¸ê¸°ì‹¬ìœ ë°œ')
        if any(word in text for word in ['ì›ƒê¸°', 'ë°”ë³´', 'ë©ì²­']): hooks.append('ìš°ì›”ê°ìê·¹')
        
        return hooks or ['ì¼ë°˜ê°ì •']
    
    def _find_viral_elements(self, item):
        """ë°”ì´ëŸ´ ìš”ì†Œ íƒì§€"""
        text = f"{item.get('title', '')} {item.get('content', '')}"
        elements = []
        
        if item.get('num_comments', 0) > 50: elements.append('ë†’ì€ì°¸ì—¬ë„')
        if any(word in text for word in ['ì‹¤í™”', 'ì§„ì§œ', 'í—']): elements.append('ì¶©ê²©ì„±')
        if len(text) < 100: elements.append('ê°„ê²°ì„±')
        
        return elements or ['ê¸°ë³¸ìš”ì†Œ']
    
    def save_training_data(self, analyzed_data):
        """ë¶„ì„ëœ ë°ì´í„°ë¥¼ í•™ìŠµìš© ë°ì´í„°ë¡œ ì €ì¥"""
        
        saved_count = 0
        
        for item in analyzed_data:
            try:
                dataset_id = self.db.insert_training_data(
                    dataset_name=f"ìŠ¤í¬ë˜í•‘_ë°ì´í„°_{item['original_data']['source']}",
                    content_type="scraped_community_data",
                    raw_data=item['original_data'],
                    processed_data=item['ai_analysis'],
                    metadata={
                        'scraping_date': datetime.now().isoformat(),
                        'analysis_cost': item['cost_used'],
                        'data_source': item['original_data']['source'],
                        'viral_score': item['original_data'].get('score', 0)
                    },
                    quality_score=item['ai_analysis'].get('effectiveness_score', 7.0)
                )
                
                saved_count += 1
                
            except Exception as e:
                logging.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                continue
        
        logging.info(f"ğŸ’¾ {saved_count}ê°œ í•™ìŠµ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        return saved_count
    
    def run_full_pipeline(self):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        logging.info("ğŸš€ í•œêµ­ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìˆ˜ì§‘ ë° AI í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logging.info(f"ğŸ’° ì‚¬ìš© ê°€ëŠ¥ ì˜ˆì‚°: ${self.ai_usage_budget}")
        
        # 1ë‹¨ê³„: ë°ì´í„° ìŠ¤í¬ë˜í•‘
        scraped_data = self.scrape_public_korean_data()
        logging.info(f"ğŸ“Š {len(scraped_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        
        # 2ë‹¨ê³„: AI ë¶„ì„
        analyzed_data = self.analyze_scraped_data_with_ai(scraped_data)
        
        # 3ë‹¨ê³„: í•™ìŠµ ë°ì´í„° ì €ì¥
        saved_count = self.save_training_data(analyzed_data)
        
        # 4ë‹¨ê³„: ê²°ê³¼ ìš”ì•½
        total_cost = self.requests_used * self.cost_per_request
        
        results = {
            'scraped_items': len(scraped_data),
            'analyzed_items': len(analyzed_data),
            'saved_items': saved_count,
            'total_cost_used': total_cost,
            'remaining_budget': self.ai_usage_budget - total_cost,
            'ai_requests_used': self.requests_used,
            'data_sources': list(set([item['original_data']['source'] for item in analyzed_data])),
            'average_effectiveness': sum([item['ai_analysis'].get('effectiveness_score', 0) for item in analyzed_data]) / len(analyzed_data) if analyzed_data else 0
        }
        
        logging.info("ğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        return results

if __name__ == "__main__":
    scraper = KoreanCommunityDataScraper()
    results = scraper.run_full_pipeline()
    
    print("\n" + "="*60)
    print("ğŸ¯ í•œêµ­ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ìˆ˜ì§‘ ë° AI í•™ìŠµ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°: {results['scraped_items']}ê°œ")
    print(f"ğŸ¤– AI ë¶„ì„ ì™„ë£Œ: {results['analyzed_items']}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ëœ í•™ìŠµ ë°ì´í„°: {results['saved_items']}ê°œ")
    print(f"ğŸ’° ì‚¬ìš©ëœ ë¹„ìš©: ${results['total_cost_used']:.2f}")
    print(f"ğŸ’° ë‚¨ì€ ì˜ˆì‚°: ${results['remaining_budget']:.2f}")
    print(f"ğŸ“ˆ í‰ê·  íš¨ê³¼ì„± ì ìˆ˜: {results['average_effectiveness']:.1f}/10")
    print(f"ğŸ” ë°ì´í„° ì†ŒìŠ¤: {', '.join(results['data_sources'])}")
    print("="*60)
