
import json
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from database_setup import TauntResearchDB
import matplotlib.pyplot as plt
import pandas as pd

class UserAnalytics:
    def __init__(self):
        self.db = TauntResearchDB()
    
    def analyze_user_patterns(self, days=30):
        """ì‚¬ìš©ì ì‚¬ìš© íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        print(f"ğŸ“Š ìµœê·¼ {days}ì¼ê°„ ì‚¬ìš©ì ë¶„ì„ ì‹œì‘...")
        
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                # ì „ì²´ ì‚¬ìš©ëŸ‰ í†µê³„
                cur.execute("""
                    SELECT 
                        COUNT(*) as total_requests,
                        COUNT(DISTINCT session_id) as unique_users,
                        AVG(response_length) as avg_response_length,
                        DATE(created_at) as date
                    FROM qa_history 
                    WHERE created_at >= %s
                    GROUP BY DATE(created_at)
                    ORDER BY date DESC;
                """, (datetime.now() - timedelta(days=days),))
                
                daily_stats = cur.fetchall()
                
                # í†¤ë³„ ì‚¬ìš© í†µê³„
                cur.execute("""
                    SELECT 
                        tone_used,
                        COUNT(*) as usage_count,
                        AVG(CAST(quality_metrics->>'readability_score' AS FLOAT)) as avg_quality,
                        AVG(response_length) as avg_length
                    FROM qa_history 
                    WHERE created_at >= %s AND tone_used IS NOT NULL
                    GROUP BY tone_used
                    ORDER BY usage_count DESC;
                """, (datetime.now() - timedelta(days=days),))
                
                tone_stats = cur.fetchall()
                
                # íƒ€ê²Ÿ ì£¼ì œ ë¶„ì„
                cur.execute("""
                    SELECT 
                        target_subject,
                        COUNT(*) as frequency,
                        tone_used,
                        AVG(response_length) as avg_length
                    FROM qa_history 
                    WHERE created_at >= %s AND target_subject IS NOT NULL
                    GROUP BY target_subject, tone_used
                    ORDER BY frequency DESC
                    LIMIT 20;
                """, (datetime.now() - timedelta(days=days),))
                
                target_stats = cur.fetchall()
        
        return {
            'daily_stats': daily_stats,
            'tone_stats': tone_stats,
            'target_stats': target_stats
        }
    
    def analyze_advanced_techniques(self):
        """ê³ ê¸‰ ê¸°ë²• ì‚¬ìš© ë¶„ì„"""
        print("ğŸ§  ê³ ê¸‰ ê¸°ë²• ì‚¬ìš© ë¶„ì„ ì‹œì‘...")
        
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                # Aposiopesis ê¸°ë²• ì‚¬ìš© í†µê³„
                cur.execute("""
                    SELECT 
                        technique_name,
                        COUNT(*) as usage_count,
                        AVG(detection_confidence) as avg_confidence,
                        AVG(effectiveness_score) as avg_effectiveness,
                        tone_used
                    FROM technique_detection_log
                    GROUP BY technique_name, tone_used
                    ORDER BY usage_count DESC;
                """)
                
                technique_stats = cur.fetchall()
                
                # ìµœê·¼ íƒì§€ëœ ê³ ê¸‰ ê¸°ë²•ë“¤
                cur.execute("""
                    SELECT 
                        tdl.technique_name,
                        tdl.detection_confidence,
                        tdl.effectiveness_score,
                        tdl.tone_used,
                        tdl.target_subject,
                        qh.user_input,
                        qh.created_at
                    FROM technique_detection_log tdl
                    JOIN qa_history qh ON tdl.qa_history_id = qh.id
                    WHERE tdl.created_at >= %s
                    ORDER BY tdl.created_at DESC
                    LIMIT 10;
                """, (datetime.now() - timedelta(days=7),))
                
                recent_techniques = cur.fetchall()
        
        return {
            'technique_stats': technique_stats,
            'recent_techniques': recent_techniques
        }
    
    def analyze_safety_patterns(self):
        """ì•ˆì „ì„± íŒ¨í„´ ë¶„ì„"""
        print("ğŸ›¡ï¸ ì•ˆì „ì„± íŒ¨í„´ ë¶„ì„ ì‹œì‘...")
        
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                # ì•ˆì „ì„± ì ê²€ ê²°ê³¼ í†µê³„
                cur.execute("""
                    SELECT 
                        CAST(safety_analysis->>'is_safe' AS BOOLEAN) as is_safe,
                        COUNT(*) as count,
                        tone_used,
                        AVG(response_length) as avg_length
                    FROM qa_history 
                    WHERE safety_analysis IS NOT NULL
                    GROUP BY CAST(safety_analysis->>'is_safe' AS BOOLEAN), tone_used
                    ORDER BY count DESC;
                """)
                
                safety_stats = cur.fetchall()
                
                # ìœ„í—˜ ìš”ì†Œ ë¶„ì„
                cur.execute("""
                    SELECT 
                        safety_analysis->>'safety_message' as safety_message,
                        COUNT(*) as frequency,
                        tone_used
                    FROM qa_history 
                    WHERE CAST(safety_analysis->>'is_safe' AS BOOLEAN) = false
                    GROUP BY safety_analysis->>'safety_message', tone_used
                    ORDER BY frequency DESC;
                """)
                
                risk_patterns = cur.fetchall()
        
        return {
            'safety_stats': safety_stats,
            'risk_patterns': risk_patterns
        }
    
    def analyze_user_preferences(self):
        """ì‚¬ìš©ì ì„ í˜¸ë„ ë¶„ì„"""
        print("â¤ï¸ ì‚¬ìš©ì ì„ í˜¸ë„ ë¶„ì„ ì‹œì‘...")
        
        with self.db.get_connection() as conn:
            with conn.cursor() as cur:
                # ì„¸ì…˜ë³„ í†¤ ì„ í˜¸ë„
                cur.execute("""
                    SELECT 
                        session_id,
                        tone_used,
                        COUNT(*) as usage_count,
                        AVG(CAST(quality_metrics->>'humor_rating' AS FLOAT)) as avg_humor_rating
                    FROM qa_history 
                    WHERE session_id IS NOT NULL AND tone_used IS NOT NULL
                    GROUP BY session_id, tone_used
                    HAVING COUNT(*) >= 2
                    ORDER BY usage_count DESC;
                """)
                
                user_preferences = cur.fetchall()
                
                # í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„
                cur.execute("""
                    SELECT 
                        unnest(keywords) as keyword,
                        COUNT(*) as frequency,
                        DATE(created_at) as date
                    FROM qa_history 
                    WHERE keywords IS NOT NULL AND array_length(keywords, 1) > 0
                    AND created_at >= %s
                    GROUP BY unnest(keywords), DATE(created_at)
                    ORDER BY frequency DESC
                    LIMIT 50;
                """, (datetime.now() - timedelta(days=30),))
                
                keyword_trends = cur.fetchall()
        
        return {
            'user_preferences': user_preferences,
            'keyword_trends': keyword_trends
        }
    
    def generate_comprehensive_report(self):
        """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        print("ğŸ“‹ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ëª¨ë“  ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘
        usage_patterns = self.analyze_user_patterns()
        technique_analysis = self.analyze_advanced_techniques()
        safety_analysis = self.analyze_safety_patterns()
        preference_analysis = self.analyze_user_preferences()
        
        # ë³´ê³ ì„œ êµ¬ì„±
        report = {
            'report_generated': datetime.now().isoformat(),
            'summary': {
                'total_users': len(set([row[1] for row in usage_patterns['daily_stats']])) if usage_patterns['daily_stats'] else 0,
                'total_requests': sum([row[0] for row in usage_patterns['daily_stats']]) if usage_patterns['daily_stats'] else 0,
                'most_popular_tone': usage_patterns['tone_stats'][0][0] if usage_patterns['tone_stats'] else 'N/A',
                'advanced_technique_usage': len(technique_analysis['technique_stats'])
            },
            'usage_patterns': usage_patterns,
            'technique_analysis': technique_analysis,
            'safety_analysis': safety_analysis,
            'preference_analysis': preference_analysis
        }
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self.generate_insights(report)
        report['insights'] = insights
        
        return report
    
    def generate_insights(self, report_data):
        """ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # í†¤ ì‚¬ìš© ì¸ì‚¬ì´íŠ¸
        if report_data['usage_patterns']['tone_stats']:
            top_tone = report_data['usage_patterns']['tone_stats'][0]
            insights.append({
                'category': 'í†¤ ì‚¬ìš© íŒ¨í„´',
                'insight': f"ê°€ì¥ ì¸ê¸°ìˆëŠ” í†¤ì€ '{top_tone[0]}'ë¡œ {top_tone[1]}íšŒ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.",
                'recommendation': f"ì´ í†¤ì˜ ì„±ê³µ ìš”ì†Œë¥¼ ë‹¤ë¥¸ í†¤ì—ë„ ì ìš©í•´ë³´ì„¸ìš”."
            })
        
        # ì•ˆì „ì„± ì¸ì‚¬ì´íŠ¸
        if report_data['safety_analysis']['safety_stats']:
            safe_ratio = sum([row[1] for row in report_data['safety_analysis']['safety_stats'] if row[0]]) / sum([row[1] for row in report_data['safety_analysis']['safety_stats']])
            insights.append({
                'category': 'ì•ˆì „ì„±',
                'insight': f"ì „ì²´ ìš”ì²­ì˜ {safe_ratio*100:.1f}%ê°€ ì•ˆì „í•œ ê²ƒìœ¼ë¡œ íŒì •ë˜ì—ˆìŠµë‹ˆë‹¤.",
                'recommendation': "ìœ„í—˜ ìš”ì†Œ íƒì§€ ì‹œìŠ¤í…œì´ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤." if safe_ratio > 0.9 else "ì•ˆì „ì„± í•„í„°ë¥¼ ê°•í™”í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."
            })
        
        # ê³ ê¸‰ ê¸°ë²• ì¸ì‚¬ì´íŠ¸
        if report_data['technique_analysis']['technique_stats']:
            advanced_usage = len(report_data['technique_analysis']['technique_stats'])
            insights.append({
                'category': 'ê³ ê¸‰ ê¸°ë²•',
                'insight': f"{advanced_usage}ê°€ì§€ ê³ ê¸‰ ê¸°ë²•ì´ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                'recommendation': "ì‚¬ìš©ìë“¤ì´ ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ë²•ì„ í™œìš©í•˜ê³  ìˆì–´ ì‹œìŠ¤í…œì´ ì„±ìˆ™í•´ì§€ê³  ìˆìŠµë‹ˆë‹¤."
            })
        
        return insights
    
    def export_report_to_file(self, report, filename=None):
        """ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"user_analytics_report_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“ ë¶„ì„ ë³´ê³ ì„œê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return filename

if __name__ == "__main__":
    analytics = UserAnalytics()
    report = analytics.generate_comprehensive_report()
    
    print("\n" + "="*60)
    print("ğŸ¯ ì¡°ë¡± í”„ë¡œì íŠ¸ ì‚¬ìš©ì ë¶„ì„ ë³´ê³ ì„œ")
    print("="*60)
    
    # ìš”ì•½ ì •ë³´ ì¶œë ¥
    summary = report['summary']
    print(f"\nğŸ“Š ì „ì²´ ìš”ì•½:")
    print(f"  â€¢ ì´ ì‚¬ìš©ì ìˆ˜: {summary['total_users']}ëª…")
    print(f"  â€¢ ì´ ìš”ì²­ ìˆ˜: {summary['total_requests']}íšŒ")
    print(f"  â€¢ ê°€ì¥ ì¸ê¸°ìˆëŠ” í†¤: {summary['most_popular_tone']}")
    print(f"  â€¢ ê³ ê¸‰ ê¸°ë²• ì‚¬ìš© ì¢…ë¥˜: {summary['advanced_technique_usage']}ê°€ì§€")
    
    # í†¤ë³„ ì‚¬ìš© í†µê³„
    if report['usage_patterns']['tone_stats']:
        print(f"\nğŸ­ í†¤ë³„ ì‚¬ìš© í†µê³„ (ìƒìœ„ 5ê°œ):")
        for i, (tone, count, quality, length) in enumerate(report['usage_patterns']['tone_stats'][:5], 1):
            print(f"  {i}. {tone}: {count}íšŒ ì‚¬ìš© (í’ˆì§ˆ: {quality:.1f}, í‰ê· ê¸¸ì´: {length:.0f}ì)")
    
    # ì•ˆì „ì„± í†µê³„
    if report['safety_analysis']['safety_stats']:
        print(f"\nğŸ›¡ï¸ ì•ˆì „ì„± í†µê³„:")
        for is_safe, count, tone, avg_length in report['safety_analysis']['safety_stats'][:3]:
            status = "ì•ˆì „" if is_safe else "ìœ„í—˜"
            print(f"  â€¢ {status}: {count}íšŒ ({tone} í†¤)")
    
    # ì¸ì‚¬ì´íŠ¸ ì¶œë ¥
    if report['insights']:
        print(f"\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
        for insight in report['insights']:
            print(f"  ğŸ” {insight['category']}: {insight['insight']}")
            print(f"     ğŸ’¬ ê¶Œì¥ì‚¬í•­: {insight['recommendation']}")
    
    # íŒŒì¼ë¡œ ì €ì¥
    filename = analytics.export_report_to_file(report)
    print(f"\nğŸ“‹ ìƒì„¸ ë³´ê³ ì„œëŠ” {filename}ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
