import json
import logging
from datetime import datetime
from typing import List, Dict, Any
import re
from collections importCounter

class NewsYoutubeTrainingProcessor:
    def __init__(self):
        self.processed_data = []

        # í”Œë«í¼ë³„ ì–¸ì–´ì  íŠ¹ì§• ë§¤í•‘
        self.platform_characteristics = {
            'naver_news': {
                'formality': 'medium_high',
                'emotion_expression': 'restrained_anger',
                'political_sensitivity': 'high',
                'criticism_style': 'direct_frustration'
            },
            'youtube': {
                'formality': 'very_low',
                'emotion_expression': 'extreme_enthusiasm',
                'slang_usage': 'very_high',
                'reaction_intensity': 'amplified'
            },
            'daum_news': {
                'formality': 'medium',
                'emotion_expression': 'empathetic_concern',
                'social_awareness': 'high',
                'tone': 'considerate'
            }
        }

        # ê°ì • ê°•ë„ë³„ í‘œí˜„ íŒ¨í„´
        self.intensity_patterns = {
            'extreme_positive': ['ì™€...', 'ì§„ì§œ êµ­ë³´ê¸‰', 'ëª…ì‘ ìŠ¤ë©œ', '1ì¼ 1ì§ìº ', 'ì•Œê³ ë¦¬ì¦˜ë‹˜ ê°ì‚¬'],
            'extreme_negative': ['ì´ê²Œ ëŒ€ì±…ì´ë¼ê³ ', 'ë¯¼ì£¼ì£¼ì˜ ë§ëƒ', 'ë‹¤ìŒ ì„ ê±° ë•Œ ë³´ì'],
            'empathetic_concern': ['ì§„ì§œ ì•„í”„ê¸´ í•œê°€ ë³´ë„¤', 'ê³ ìƒ ë§ìœ¼ì‹­ë‹ˆë‹¤', 'ì •ë¶€ëŠ” ëŒ€ì±… ì¢€'],
            'humorous_understanding': ['ê°œì›ƒê¸°ë„¤ ì§„ì§œ', 'ê³µê°ì€ ê°„ë‹¤', 'ì„œë¡œ ì´í•´í•˜ë ¤ëŠ” ë…¸ë ¥'],
            'social_despair': ['ìƒëŒ€ì  ë°•íƒˆê°', 'ì„œë¯¼ë“¤ì€ í•œ í‰ìƒ', 'ì´ëŸ° ê¸°ì‚¬ ì•ˆ ë³´ê³  ì‹¶ë‹¤']
        }

        self.platform_patterns = {
            'naver_news': {
                'characteristics': ['ëƒ‰ì†Œì ', 'ì •ì¹˜ì ', 'í˜„ì‹¤ì '],
                'common_phrases': ['ì´ê²Œ ëŒ€ì±…ì´ë¼ê³ ', 'ì„œë¯¼ë“¤ì€', 'ë‹¤ìŒ ì„ ê±° ë•Œ']
            },
            'youtube': {
                'characteristics': ['ê°íƒ„ì‚¬', 'ê·¹ì°¬', 'ë°ˆ ë¬¸í™”'],
                'common_phrases': ['êµ­ë³´ê¸‰', 'ëª…ì‘ ìŠ¤ë©œ', 'ì•Œê³ ë¦¬ì¦˜ë‹˜ ê°ì‚¬']
            },
            'daum_news': {
                'characteristics': ['ê°ì •ì ', 'ê³µê°ì ', 'ê±±ì •'],
                'common_phrases': ['ë§ˆìŒì´', 'ê³ ìƒ ë§ìœ¼ì‹­ë‹ˆë‹¤', 'ì¡°ì‹¬í•˜ì„¸ìš”']
            }
        }

    def analyze_comment_psychology(self, comment_data: Dict[str, Any]) -> Dict[str, Any]:
        """ëŒ“ê¸€ì˜ ì‹¬ë¦¬ì  ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„"""
        content = comment_data.get('content', '')
        speech_pattern = comment_data.get('speech_pattern', '')
        emotional_intensity = comment_data.get('emotional_intensity', 0)
        stance = comment_data.get('stance', 'neutral')

        # ì‹¬ë¦¬ì  ë™ê¸° ë¶„ì„
        psychological_drivers = []

        if stance == 'strong_negative' and emotional_intensity > 9:
            psychological_drivers.extend(['ë¶„ë…¸_í‘œì¶œ', 'ì •ì¹˜ì _ì¢Œì ˆê°', 'ì‹œìŠ¤í…œ_ë¶ˆì‹ '])
        elif stance == 'strong_positive' and emotional_intensity > 9:
            psychological_drivers.extend(['íŒ¬ë¤_ì†Œì†ê°', 'ì§‘ë‹¨_ì •ì²´ì„±', 'ê°ì •_ê³¼ëª°ì…'])
        elif 'empathetic' in speech_pattern:
            psychological_drivers.extend(['ì‚¬íšŒì _ê³µê°', 'íƒ€ì¸_ë°°ë ¤', 'ì§‘ë‹¨_ì—°ëŒ€'])
        elif 'humorous' in speech_pattern:
            psychological_drivers.extend(['ê¸´ì¥_ì™„í™”', 'ì„¸ëŒ€_ì´í•´', 'ê· í˜•_ê°ê°'])

        # ì–¸ì–´ì  ì „ëµ ë¶„ì„
        linguistic_strategies = self._extract_linguistic_strategies(content, speech_pattern)

        # ë°”ì´ëŸ´ ìš”ì†Œ ì‹ë³„
        viral_elements = self._identify_viral_elements(comment_data)

        return {
            'psychological_drivers': psychological_drivers,
            'linguistic_strategies': linguistic_strategies,
            'viral_elements': viral_elements,
            'engagement_prediction': self._predict_engagement(comment_data)
        }

    def _extract_linguistic_strategies(self, content: str, speech_pattern: str) -> List[str]:
        """ì–¸ì–´ì  ì „ëµ ì¶”ì¶œ"""
        strategies = []

        # ìˆ˜ì‚¬ ê¸°ë²• ë¶„ì„
        if '...' in content:
            strategies.append('ë§ì¤„ì„_ì—¬ìš´')
        if re.search(r'ã…‹{2,}', content):
            strategies.append('ì›ƒìŒ_í‘œí˜„_ì¦í­')
        if '?' in content and 'ë§ëƒ' in content:
            strategies.append('ë°˜ë¬¸ë²•_ì••ë°•')
        if 'ì§„ì§œ' in content or 'ì™„ì „' in content:
            strategies.append('ê°•í™”ì–´_ì‚¬ìš©')
        if 'ë‹˜' in content:
            strategies.append('ì¡´ëŒ“ë§_ì¹œê·¼ê°')

        # í”Œë«í¼ë³„ íŠ¹ì§•
        if 'youtube' in speech_pattern:
            strategies.extend(['ê°íƒ„ì‚¬_ê³¼ìš©', 'êµ¬ë…_ìœ ë„', 'ì•Œê³ ë¦¬ì¦˜_ì–¸ê¸‰'])
        elif 'news' in speech_pattern:
            strategies.extend(['ì •ì¹˜ì _ë¹„íŒ', 'ì‚¬íšŒ_í˜„ì‹¤_ì§€ì ', 'ì •ì±…_í‰ê°€'])

        return strategies

    def _identify_viral_elements(self, comment_data: Dict[str, Any]) -> List[str]:
        """ë°”ì´ëŸ´ ìš”ì†Œ ì‹ë³„"""
        viral_elements = []

        score = comment_data.get('score', 0)
        comments = comment_data.get('num_comments', 0)
        content = comment_data.get('content', '')

        # ìˆ˜ì¹˜ ê¸°ë°˜ ë°”ì´ëŸ´ ìš”ì†Œ
        if score > 10000:
            viral_elements.append('ì´ˆê³ ì°¸ì—¬ë„')
        if comments > 2000:
            viral_elements.append('ë…¼ìŸ_ìœ ë°œì„±')

        # ë‚´ìš© ê¸°ë°˜ ë°”ì´ëŸ´ ìš”ì†Œ
        if any(word in content for word in ['êµ­ë³´ê¸‰', 'ëª…ì‘', 'ë ˆì „ë“œ']):
            viral_elements.append('ê·¹ì°¬_í‘œí˜„')
        if any(word in content for word in ['ë¯¼ì£¼ì£¼ì˜', 'ì„ ê±°', 'ì •ë¶€']):
            viral_elements.append('ì •ì¹˜ì _ë…¼ë€ì„±')
        if 'ê³µê°' in content or 'ì´í•´' in content:
            viral_elements.append('ê³µê°ëŒ€_í˜•ì„±')

        return viral_elements

    def _predict_engagement(self, comment_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì°¸ì—¬ë„ ì˜ˆì¸¡"""
        emotional_intensity = comment_data.get('emotional_intensity', 0)
        data_type = comment_data.get('data_type', '')
        stance = comment_data.get('stance', 'neutral')

        # ì°¸ì—¬ë„ ì˜ˆì¸¡ ëª¨ë¸
        base_score = emotional_intensity * 10

        # ë°ì´í„° íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        type_multipliers = {
            'political_opposition': 1.5,
            'fandom_worship': 1.3,
            'generational_humor': 1.2,
            'social_criticism': 1.1,
            'entertainment_reaction': 1.0
        }

        multiplier = type_multipliers.get(data_type, 1.0)
        predicted_score = base_score * multiplier

        return {
            'predicted_engagement_score': predicted_score,
            'viral_probability': min(predicted_score / 100, 1.0),
            'controversy_level': 'high' if 'negative' in stance else 'medium'
        }

    def generate_tone_mapping(self, comment_data: Dict[str, Any]) -> List[str]:
        """ëŒ“ê¸€ íŠ¹ì„±ì— ë§ëŠ” í†¤ ë§¤í•‘"""
        speech_pattern = comment_data.get('speech_pattern', '')
        data_type = comment_data.get('data_type', '')
        stance = comment_data.get('stance', 'neutral')
        emotional_intensity = comment_data.get('emotional_intensity', 0)

        tone_recommendations = []

        # íŒ¨í„´ë³„ í†¤ ë§¤í•‘
        if 'cynical' in speech_pattern:
            tone_recommendations.extend(['ëƒ‰ì†Œ í†¤', 'ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ë°•í•˜ëŠ”', 'í’ìì '])
        elif 'praise' in speech_pattern:
            tone_recommendations.extend(['ì• êµ í†¤', 'ê°ì„± ì—ì„¸ì´ í†¤', 'ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ'])
        elif 'empathetic' in speech_pattern:
            tone_recommendations.extend(['ì—ê²í†¤', 'ê°ì„± ì—ì„¸ì´ í†¤'])
        elif 'aggressive' in speech_pattern:
            tone_recommendations.extend(['ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ë°•í•˜ëŠ”', 'ëƒ‰ì†Œ í†¤'])
        elif 'fandom' in speech_pattern:
            tone_recommendations.extend(['ì• êµ í†¤', 'MZ ë°˜ë§ í†¤', 'ì •ì‹ ë‚˜ê°„ í†¤'])
        elif 'relatable' in speech_pattern:
            tone_recommendations.extend(['MZ ë°˜ë§ í†¤', 'ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ'])

        # ê°ì • ê°•ë„ë³„ ì¶”ê°€ í†¤
        if emotional_intensity > 9:
            tone_recommendations.extend(['ì •ì‹ ë‚˜ê°„ í†¤', 'ì†Œì‹¬í•œ ê³µê²© í†¤'])
        elif emotional_intensity > 7:
            tone_recommendations.extend(['í’ìì ', 'ë¹„ê¼¬ëŠ” ë“¯ì´'])

        return list(set(tone_recommendations))

    def process_news_youtube_data(self, raw_data: List[Dict]) -> List[Dict]:
        """ë‰´ìŠ¤/ìœ íŠœë¸Œ ëŒ“ê¸€ ë°ì´í„°ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ë³€í™˜"""
        processed_data = []

        for item in raw_data:
            processed_item = {
                'raw_data': item,
                'processed_at': datetime.now().isoformat(),
                'platform_type': self._identify_platform(item.get('source', '')),
                'speech_pattern': item.get('speech_pattern', 'unknown'),
                'emotional_intensity': item.get('emotional_intensity', 5.0),
                'psychological_drivers': self._extract_psychological_drivers(item),
                'viral_potential': self._calculate_viral_potential(item),
                'recommended_adaptations': self._suggest_adaptations(item)
            }
            processed_data.append(processed_item)

        return processed_data

    def _extract_platform(self, source: str) -> str:
        """ì†ŒìŠ¤ì—ì„œ í”Œë«í¼ ì¶”ì¶œ"""
        if 'naver' in source:
            return 'naver_news'
        elif 'youtube' in source:
            return 'youtube'
        elif 'daum' in source:
            return 'daum_news'
        return 'unknown'

    def _identify_platform(self, source: str) -> str:
        """ì†ŒìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”Œë«í¼ì„ ì‹ë³„í•©ë‹ˆë‹¤."""
        if 'naver' in source:
            return 'naver_news'
        elif 'youtube' in source:
            return 'youtube'
        elif 'daum' in source:
            return 'daum_news'
        else:
            return 'unknown'

    def _extract_psychological_drivers(self, item: Dict) -> List[str]:
        """ì‹¬ë¦¬ì  ë™ê¸° ìš”ì†Œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        drivers = []
        content = item.get('content', '').lower()
        stance = item.get('stance', 'neutral')

        # ì‹¬ë¦¬ì  ë™ê¸° íŒ¨í„´
        if stance == 'negative':
            drivers.append('frustration_release')
        elif stance == 'positive':
            drivers.append('validation_seeking')

        if 'ì§„ì§œ' in content or 'ì™„ì „' in content:
            drivers.append('emphasis_need')
        if 'ê°ì‚¬' in content or 'ê³ ë§ˆ' in content:
            drivers.append('gratitude_expression')
        if 'ê±±ì •' in content or 'ì¡°ì‹¬' in content:
            drivers.append('care_showing')

        return drivers

    def _calculate_viral_potential(self, item: Dict) -> float:
        """ë°”ì´ëŸ´ ì ì¬ë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        score = item.get('score', 0)
        comments = item.get('num_comments', 0)
        emotional_intensity = item.get('emotional_intensity', 5.0)

        # ê°€ì¤‘ì¹˜ ì ìš©í•œ ë°”ì´ëŸ´ ì ìˆ˜
        viral_score = (score * 0.4 + comments * 0.3 + emotional_intensity * 1000 * 0.3) / 10000
        return min(viral_score, 1.0)

    def _suggest_adaptations(self, item: Dict) -> List[str]:
        """ì ì‘ ë°©ì•ˆì„ ì œì•ˆí•©ë‹ˆë‹¤."""
        adaptations = []
        platform = self._identify_platform(item.get('source', ''))

        if platform == 'naver_news':
            adaptations.extend(['ëƒ‰ì†Œ í†¤', 'ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ë°•í•˜ëŠ”'])
        elif platform == 'youtube':
            adaptations.extend(['ìœ ë¨¸ëŸ¬ìŠ¤í•˜ê²Œ', 'MZ ë°˜ë§ í†¤'])
        elif platform == 'daum_news':
            adaptations.extend(['ê°ì„± ì—ì„¸ì´ í†¤', 'ì—ê²í†¤'])

        return adaptations[:2]  # ìµœëŒ€ 2ê°œ

    def generate_insights(self, processed_data: List[Dict]) -> Dict[str, Any]:
        """ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = {
            'total_samples': len(processed_data),
            'platform_distribution': {},
            'emotional_intensity_stats': {},
            'top_psychological_drivers': [],
            'recommended_tones': {},
            'viral_potential_analysis': {}
        }

        # í”Œë«í¼ ë¶„í¬
        platform_counts = Counter([sample['raw_data']['platform'] for sample in processed_data])
        insights['platform_distribution'] = dict(platform_counts)

        # ê°ì • ê°•ë„ í†µê³„
        intensities = [sample['raw_data']['emotional_intensity'] for sample in processed_data]
        insights['emotional_intensity_stats'] = {
            'average': sum(intensities) / len(intensities) if intensities else 0,
            'max': max(intensities) if intensities else 0,
            'extreme_count': len([i for i in intensities if i > 9])
        }

        # ì‹¬ë¦¬ì  ë™ê¸° ë¶„ì„
        all_drivers = []
        for sample in processed_data:
            all_drivers.extend(sample['processed_data']['psychology_analysis']['psychological_drivers'])
        insights['top_psychological_drivers'] = Counter(all_drivers).most_common(10)

        # í†¤ ì¶”ì²œ ë¶„ì„
        all_tones = []
        for sample in processed_data:
            all_tones.extend(sample['processed_data']['tone_recommendations'])
        insights['recommended_tones'] = Counter(all_tones).most_common(15)

        # ë°”ì´ëŸ´ ì ì¬ë ¥ ë¶„ì„
        viral_scores = [sample['processed_data']['viral_potential'] for sample in processed_data]
        insights['viral_potential_analysis'] = {
            'average_viral_score': sum(viral_scores) / len(viral_scores) if viral_scores else 0,
            'high_viral_count': len([v for v in viral_scores if v > 0.7])
        }

        return insights

    def generate_insights(self, processed_data: List[Dict]) -> Dict:
        """ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        total_samples = len(processed_data)

        # í”Œë«í¼ ë¶„í¬
        platform_distribution = {}
        for item in processed_data:
            platform = item['platform_type']
            platform_distribution[platform] = platform_distribution.get(platform, 0) + 1

        # ì‹¬ë¦¬ì  ë™ê¸° ë¶„ì„
        psychological_drivers = {}
        for item in processed_data:
            for driver in item['psychological_drivers']:
                psychological_drivers[driver] = psychological_drivers.get(driver, 0) + 1

        # ë°”ì´ëŸ´ ì ì¬ë ¥ ë¶„ì„
        viral_scores = [item['viral_potential'] for item in processed_data]
        high_viral_threshold = 0.7
        high_viral_count = len([score for score in viral_scores if score > high_viral_threshold])

        # ê°ì • ê°•ë„ í†µê³„
        emotional_intensities = [item['emotional_intensity'] for item in processed_data]
        extreme_threshold = 8.5
        extreme_count = len([intensity for intensity in emotional_intensities if intensity > extreme_threshold])

        # ì¶”ì²œ í†¤ ë¶„ì„
        recommended_tones = {}
        for item in processed_data:
            for tone in item['recommended_adaptations']:
                recommended_tones[tone] = recommended_tones.get(tone, 0) + 1

        return {
            'total_samples': total_samples,
            'platform_distribution': platform_distribution,
            'top_psychological_drivers': sorted(psychological_drivers.items(), key=lambda x: x[1], reverse=True),
            'viral_potential_analysis': {
                'average_viral_score': sum(viral_scores) / len(viral_scores) if viral_scores else 0,
                'high_viral_count': high_viral_count,
                'viral_rate': high_viral_count / total_samples if total_samples > 0 else 0
            },
            'emotional_intensity_stats': {
                'average': sum(emotional_intensities) / len(emotional_intensities) if emotional_intensities else 0,
                'extreme_count': extreme_count,
                'extreme_rate': extreme_count / total_samples if total_samples > 0 else 0
            },
            'recommended_tones': sorted(recommended_tones.items(), key=lambda x: x[1], reverse=True)
        }

def process_news_youtube_training_data():
    """ë‰´ìŠ¤/ìœ íŠœë¸Œ ëŒ“ê¸€ í•™ìŠµ ë°ì´í„° ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    processor = NewsYoutubeTrainingProcessor()

    # ì²¨ë¶€ëœ ë°ì´í„° ë¡œë“œ
    comment_data = [
        {
            "source": "simulated_naver_news_comment",
            "title": "[ì†ë³´] ì •ë¶€, 3ê¸° ì‹ ë„ì‹œ ì¶”ê°€ ê³µê¸‰ ë° DSR ê·œì œ ì™„í™” ë°œí‘œ",
            "content": "ì´ê²Œ ëŒ€ì±…ì´ë¼ê³  ë‚´ë†“ì€ê±´ê°€? ì§‘ê°’ ì¡ì„ ìƒê°ì€ ì—†ê³  ê·¸ëƒ¥ ê±´ì„¤ì‚¬ë“¤ ë°°ë§Œ ë¶ˆë ¤ì£¼ìëŠ” ê±°ì–ì•„. ì„œë¯¼ë“¤ì€ ì–´ì°¨í”¼ ëŒ€ì¶œë„ ì•ˆë‚˜ì™€ì„œ ê·¸ë¦¼ì˜ ë–¡ì„.",
            "score": 5820,
            "num_comments": 1250,
            "data_type": "policy_criticism",
            "speech_pattern": "news_comment_cynical",
            "emotional_intensity": 9.3,
            "stance": "negative"
        },
        {
            "source": "simulated_youtube_comment",
            "title": "ì˜í™” 'ê´‘í•´ 2' ì˜ˆê³ í¸ ìµœì´ˆ ê³µê°œ! ë°°ìš° ì´ë³‘í—Œ 1ì¸ 2ì—­ ë³µê·€",
            "content": "ì™€... ì˜ˆê³ í¸ë§Œ ë´¤ëŠ”ë° ë²Œì¨ ëª…ì‘ ìŠ¤ë©œì´ ë‚œë‹¤. ì´ë³‘í—Œ ì—°ê¸°ëŠ” ì§„ì§œ êµ­ë³´ê¸‰ì´ë„¤. ì²œë§Œ ê´€ê° ê·¸ëƒ¥ ë„˜ì„ ë“¯ ã„·ã„·",
            "score": 12000,
            "num_comments": 3400,
            "data_type": "entertainment_reaction",
            "speech_pattern": "youtube_comment_praise",
            "emotional_intensity": 9.0,
            "stance": "positive"
        },
        {
            "source": "simulated_daum_news_comment",
            "title": "ì—­ëŒ€ê¸‰ í­ì—¼ì— ì „ë ¥ìˆ˜ê¸‰ 'ê²½ê³ 'â€¦ 7ì›”ì¸ë° ë²Œì¨ 38ë„",
            "content": "ì§€êµ¬ê°€ ì§„ì§œ ì•„í”„ê¸´ í•œê°€ ë³´ë„¤ìš”... ë‹¤ë“¤ ë”ìœ„ ì¡°ì‹¬í•˜ì‹œê³ , íŠ¹íˆ ì•¼ì™¸ì—ì„œ ì¼í•˜ì‹œëŠ” ë¶„ë“¤ ì •ë§ ê³ ìƒ ë§ìœ¼ì‹­ë‹ˆë‹¤. ì •ë¶€ëŠ” ì „ê¸°ì„¸ ì§€ì› ê°™ì€ ëŒ€ì±… ì¢€ ì„¸ì›Œì£¼ì„¸ìš”.",
            "score": 3500,
            "num_comments": 880,
            "data_type": "social_concern",
            "speech_pattern": "news_comment_empathetic",
            "emotional_intensity": 7.5,
            "stance": "concerned_neutral"
        },
        {
            "source": "simulated_youtube_comment",
            "title": "ìš”ì¦˜ MZ ì‹ ì…ì‚¬ì› íŠ¹ì§•.mp4 (feat. ë¼ë–¼ëŠ” ë§ì´ì•¼)",
            "content": "ã…‹ã…‹ã…‹ã…‹ã…‹ ê°œì›ƒê¸°ë„¤ ì§„ì§œ ìš°ë¦¬ íšŒì‚¬ ë¶€ì¥ë‹˜ ë³´ëŠ” ì¤„. ê·¼ë° ì†”ì§íˆ ì„œë¡œ ì´í•´í•˜ë ¤ëŠ” ë…¸ë ¥ì´ í•„ìš”í•¨. ì €ë ‡ê²Œê¹Œì§€ í•˜ëŠ” ì‹ ì…ì€ ì—†ì§€ë§Œ ì–´ëŠ ì •ë„ ê³µê°ì€ ê°„ë‹¤.",
            "score": 8800,
            "num_comments": 2100,
            "data_type": "generational_humor",
            "speech_pattern": "youtube_comment_relatable",
            "emotional_intensity": 8.2,
            "stance": "humorous_neutral"
        },
        {
            "source": "simulated_naver_news_comment",
            "title": "ë…¼ë€ì˜ 'OOOë²•' êµ­íšŒ í†µê³¼â€¦ ì‹œë¯¼ë‹¨ì²´ ê°•ë ¥ ë°˜ë°œ",
            "content": "ì´ê²Œ ë¯¼ì£¼ì£¼ì˜ êµ­ê°€ ë§ëƒ? êµ­ë¯¼ ì˜ê²¬ì€ ì‹¹ ë‹¤ ë¬´ì‹œí•˜ê³  ê·¸ëƒ¥ ë°€ì–´ë¶™ì´ë„¤. ë‹¤ìŒ ì„ ê±° ë•Œ ë³´ì.",
            "score": 7600,
            "num_comments": 3200,
            "data_type": "political_opposition",
            "speech_pattern": "news_comment_aggressive",
            "emotional_intensity": 9.8,
            "stance": "strong_negative"
        },
        {
            "source": "simulated_youtube_comment",
            "title": "[4K ì§ìº ] XXX ì•„ì´ëŒ ì‹ ê³¡ 'FANTASY' ì‡¼ì¼€ì´ìŠ¤ ë¬´ëŒ€",
            "content": "ì•Œê³ ë¦¬ì¦˜ë‹˜, ì €ë¥¼ ì´ê³³ìœ¼ë¡œ ì¸ë„í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤... ë§¤ì¼ ë³´ëŸ¬ ì˜¤ê² ìŠµë‹ˆë‹¤. 1ì¼ 1ì§ìº  í•„ìˆ˜.",
            "score": 25000,
            "num_comments": 5500,
            "data_type": "fandom_worship",
            "speech_pattern": "youtube_comment_fandom",
            "emotional_intensity": 9.5,
            "stance": "strong_positive"
        },
        {
            "source": "simulated_daum_news_comment",
            "title": "[ë‹¨ë…] ìœ ëª… ì—°ì˜ˆì¸ OOO, 100ì–µëŒ€ ê±´ë¬¼ ë§¤ì…",
            "content": "ì´ëŸ° ê¸°ì‚¬ ì¢€ ì•ˆ ë³´ê³  ì‹¶ë‹¤. ìƒëŒ€ì  ë°•íƒˆê°ë§Œ ë“œë„¤. ì„œë¯¼ë“¤ì€ í•œ í‰ìƒ ëª¨ì•„ë„ ëŒ€ì¶œ ê°šê¸° í˜ë“ ë°...",
            "score": 4100,
            "num_comments": 1500,
            "data_type": "social_criticism",
            "speech_pattern": "news_comment_despair",
            "emotional_intensity": 8.0,
            "stance": "negative"
        },
        {
            "source": "simulated_youtube_comment",
            "title": "10ë¶„ë§Œì— ì´í•´í•˜ëŠ” ì–‘ìì—­í•™",
            "content": "ì™€... ì„¤ëª…ì„ ë„ˆë¬´ ì˜í•´ì£¼ì…”ì„œ ë¬¸ê³¼ìƒì¸ë° ì²˜ìŒìœ¼ë¡œ ì´í•´í–ˆì–´ìš”. 10ë¶„ ìˆœì‚­ì´ë„¤ìš”. êµ¬ë…í•˜ê³  ê°‘ë‹ˆë‹¤!",
            "score": 15000,
            "num_comments": 2800,
            "data_type": "educational_feedback",
            "speech_pattern": "youtube_comment_appreciation",
            "emotional_intensity": 7.0,
            "stance": "positive"
        }
    ]

    # ë°ì´í„° ì²˜ë¦¬
    processed_data = processor.process_news_youtube_data(comment_data)

    # ì¸ì‚¬ì´íŠ¸ ìƒì„±
    insights = processor.generate_insights(processed_data)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“º ë‰´ìŠ¤/ìœ íŠœë¸Œ ëŒ“ê¸€ í•™ìŠµ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
    print("="*60)

    print(f"\nğŸ“ˆ ì²˜ë¦¬ ê²°ê³¼:")
    print(f"  â€¢ ì´ í•™ìŠµ ìƒ˜í”Œ: {insights['total_samples']}ê°œ")
    print(f"  â€¢ í‰ê·  ê°ì • ê°•ë„: {insights['emotional_intensity_stats']['average']:.2f}")
    print(f"  â€¢ ê·¹ê°• ê°ì • ëŒ“ê¸€: {insights['emotional_intensity_stats']['extreme_count']}ê°œ")
    print(f"  â€¢ í‰ê·  ë°”ì´ëŸ´ ì ìˆ˜: {insights['viral_potential_analysis']['average_viral_score']:.3f}")

    print(f"\nğŸŒ í”Œë«í¼ ë¶„í¬:")
    for platform, count in insights['platform_distribution'].items():
        print(f"  â€¢ {platform}: {count}ê°œ")

    print(f"\nğŸ§  ìƒìœ„ ì‹¬ë¦¬ì  ë™ê¸°:")
    for driver, count in insights['top_psychological_drivers'][:5]:
        print(f"  â€¢ {driver}: {count}íšŒ")

    print(f"\nğŸ­ ì¶”ì²œ í†¤ ë¶„í¬:")
        for tone, count in insights['recommended_tones'][:5]:
            print(f"  â€¢ {tone}: {count}íšŒ")

        return processed_data, insights

if __name__ == "__main__":
    process_news_youtube_training_data()